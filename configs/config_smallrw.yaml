dataset_name: small_rw
axis: 2  # For lambda baseline this has to be 1.
batch_size: 128
learning_rate: .1
use_quantiles: False #used for mtlr quantile time bins
log_interval: 1000
weight_decay: 0
num_epochs: 100
output_file: Results/{}/
landmark: True
target_lr: .5
layer_size: 16
num_hidden: 1
arch:
  type: linear
  arch_kwargs:
    hidden_size: 16
    seq_len: 11  # Has to be the same as the horizon
    output_dim: 8
    num_layers: 1
    dropout: 0.05

preprocessed_data: False
dataset_kwargs:
  horizon: 11
  data_path: data/SmallRW.pkl

lambda_: 1.0
C1: 1.0
num_steps: 5
verbose: False
num_trials: 5

num_seqs: [10, 25, 50, 100, 150, 200]

# hyperparams:
#   SA:
#     names: [learning_rate]
#     vals: [[1.e-1]]
#   LambdaSA:
#     names: [learning_rate, num_steps]
#     vals: [[1.e-1], [5]] 
#   DeepLambdaSA:
#     names: [learning_rate, target_lr, lambda_]
#     vals: [[1.e-1], [1.e-1], [0]]
#   MTLR:
#     names: [learning_rate, use_quantiles, C1]
#     vals: [[1.e-3], [True], [0]]
#   TC_MTLR:
#     names: [learning_rate, use_quantiles, tau, lambda_]
#     vals: [[1.e-3], [True], [5.e-1], [0]]

hyperparams:
  SA:
    names: [learning_rate]
    vals: [[1.e-1, 1.e-2, 1.e-3, 1.e-4]]
  LambdaSA:
    names: [learning_rate, num_steps]
    vals: [[1.e-1, 1.e-2, 1.e-3, 1.e-4], [5, 10, 20]] 
  DeepLambdaSA:
    names: [learning_rate, target_lr, lambda_]
    vals: [[1.e-1, 1.e-2, 1.e-3, 1.e-4], [1, 5.e-1, 1.e-1], [0]]
  MTLR:
    names: [learning_rate, use_quantiles, C1]
    vals: [[1.e-3, 1.e-4, 1.e-5], [True, False], [1.0, 0.1, 0.0]]
  TC_MTLR:
    names: [learning_rate, use_quantiles, tau, lambda_]
    vals: [[1.e-3, 1.e-4, 1.e-5], [True, False], [1, 5.e-1, 1.e-1], [0]]
