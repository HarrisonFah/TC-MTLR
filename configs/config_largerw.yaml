dataset_name: large_rw
axis: 2  # For lambda baseline this has to be 1.
batch_size: 128
learning_rate: .1
use_quantiles: False #used for mtlr quantile time bins
log_interval: 1000
weight_decay: 1.e-4
num_epochs: 1000
output_file: Results/{}/
landmark: True
target_lr: .5
layer_size: 16
num_hidden: 1
arch:
  type: transformer
  arch_kwargs:
    hidden_size: 16
    seq_len: 100  # Has to be the same as the horizon
    output_dim: 8
    num_layers: 1
    dropout: 0.05
# arch:
#   type: linear
#   arch_kwargs:
#     hidden_size: 16
#     seq_len: 100  # Has to be the same as the horizon
#     output_dim: 8
#     num_layers: 1
#     dropout: 0.05

preprocessed_data: False
dataset_kwargs:
  horizon: 100
  data_path: data/LargeRW.pkl

lambda_: 1.0
C1: 1.0
num_steps: 5
verbose: False
num_trials: 5

num_seqs: [1000]
#num_seqs: []

hyperparams:
  SA:
    names: [learning_rate]
    vals: [[1.e-2]]
  LambdaSA:
    names: [learning_rate, num_steps]
    vals: [[1.e-2], [10]] 
  DeepLambdaSA:
    names: [learning_rate, target_lr, lambda_]
    vals: [[1.e-2], [5.e-1], [0, 0.9, 0.95]]
  MTLR:
    names: [learning_rate, use_quantiles, C1]
    vals: [[1.e-4], [True], [1.0]]
  TC_MTLR:
    names: [learning_rate, use_quantiles, policy_freq]
    vals: [[1.e-4], [True], [10]]

# hyperparams:
#   SA:
#     names: [learning_rate]
#     vals: [[1.e-1, 1.e-2, 1.e-3]]
#   LambdaSA:
#     names: [learning_rate, num_steps]
#     vals: [[1.e-2], [10]] 
#   DeepLambdaSA:
#     names: [learning_rate, target_lr, lambda_]
#     vals: [[1.e-1, 1.e-2, 1.e-3], [5.e-1], [0, 0.9, 0.95]]
#   MTLR:
#     names: [learning_rate, use_quantiles, C1]
#     vals: [[1.e-3, 1.e-4, 1.e-5], [True, False], [1.0]]
#   TC_MTLR:
#     names: [learning_rate, use_quantiles, policy_freq]
#     vals: [[1.e-3, 1.e-4, 1.e-5], [True, False], [10]]
